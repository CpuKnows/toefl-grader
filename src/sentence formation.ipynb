{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence formation error detection\n",
    "Part c.iii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# This uses corenlp server! Will need to alter code if using JAR files directly\n",
    "# https://stanfordnlp.github.io/CoreNLP/corenlp-server.html\n",
    "from nltk.parse.corenlp import CoreNLPParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constituency_parse(parser, text, return_parse_obj=False, properties=None):\n",
    "    \"\"\"Creates parse strings for text.  \n",
    "    Each parse string can be fed into Tree.fromstring() to create NLTK Tree objects.\n",
    "\n",
    "    parser (CoreNLPParser): parser to parse sentences\n",
    "    text (str): essay text\n",
    "    return_parse_obj (bool): return parse object or string of trees\n",
    "    properties (dict): override or add CoreNLP properties\n",
    "    RETURNS (list/dict): a list of parses in string form or parse dict depending on return_parse_obj parameter\n",
    "    \"\"\"\n",
    "    default_properties = {'annotators': 'tokenize,pos,ssplit,parse', \n",
    "                          'parse.buildgraphs': 'false'}\n",
    "                          #'parse.model': 'edu/stanford/nlp/models/srparser/englishSR.ser.gz'}\n",
    "    default_properties.update(properties or {})\n",
    "\n",
    "    parsed_data = parser.api_call(text, properties=default_properties)\n",
    "    if return_parse_obj:\n",
    "        return parsed_data\n",
    "    else:\n",
    "        parses = list()\n",
    "        for parse in parsed_data['sentences']:\n",
    "            parse = parse['parse']\n",
    "            # Compress whitespace\n",
    "            parse = re.sub('[\\s]+', ' ', parse)\n",
    "            parses.append(parse)\n",
    "        return parses\n",
    "    \n",
    "def tree_to_str(trees):\n",
    "    \"\"\"Joins a list of trees in string form\"\"\"\n",
    "    return ' '.join(trees)\n",
    "\n",
    "def str_to_trees(tree_str):\n",
    "    \"\"\"Splits a string into a list of trees in string form\"\"\"\n",
    "    d = \"(ROOT\"\n",
    "    return  [(d+e).strip() for e in tree_str.split(d) if e]\n",
    "\n",
    "def get_productions(tree):\n",
    "    \"\"\"Get productions from an NLTK Tree object.  \n",
    "    return a list of production rule strings.\"\"\"\n",
    "    rules = list()\n",
    "\n",
    "    for rule in tree.productions():\n",
    "        if not rule.is_lexical() and 'ROOT' not in rule.unicode_repr():\n",
    "            rules.append(rule.unicode_repr())\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence formation funcions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run tree_utils.py\n",
    "# Imports tree functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_formation_errors(parsed_sentence):\n",
    "    \"\"\"Finds sentence formation errors in a parsed sentence\n",
    "    \n",
    "    parsed_sentences (str): CoreNLP constituency parser output\n",
    "    RETURNS (dict): Dict of error types with boolean values for if they occur in the sentence\"\"\"\n",
    "    errors = {}\n",
    "    root_node = create_tree(parsed_sentence)\n",
    "    \n",
    "    # fragment\n",
    "    if 'FRAG' in root_node:\n",
    "        errors['fragment'] = True\n",
    "    \n",
    "    \n",
    "    for node in root_node.get_descendants('SBAR'):\n",
    "        # SBAR without S\n",
    "        if len(list(node.get_ancestors('S'))) == 0:\n",
    "            errors['sbar_without_s'] = True\n",
    "            \n",
    "        # because with VBG\n",
    "        if (node.children[0].label == 'IN' and node.children[0].word.lower() == 'because' and \n",
    "            'S' in [s.label for s in node.children[0].get_right_siblings()]):\n",
    "            # Get first leftmost VP\n",
    "            vp_node = node.get_descendants('VP').__next__()\n",
    "            if 'VBG' in [c.label for c in vp_node.children]:\n",
    "                errors['sbar_with_vbg'] = True\n",
    "    \n",
    "        # SBAR withot CC\n",
    "        \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essay_sentence_formation_errors(parser, essay_text):\n",
    "    \"\"\"Parses raw text to find sentence formation errors. Returns the number of sentences with at least 1 error\n",
    "    \n",
    "    Each parse string can be fed into Tree.fromstring() to create NLTK Tree objects.\n",
    "\n",
    "    parser (CoreNLPParser): parser to parse sentences\n",
    "    essay_text (str): essay text\n",
    "    RETURNS (int, int): number of sentences with formation erros, total number of sentences\n",
    "    \"\"\"\n",
    "    parsed_sentences = constituency_parse(parser, essay_text)\n",
    "    \n",
    "    count_sentences_with_errors = 0\n",
    "    for sent in parsed_sentences:\n",
    "        errors = sentence_formation_errors(sent)\n",
    "        if len(errors) > 0:\n",
    "            count_sentences_with_errors += 1\n",
    "        \n",
    "    return count_sentences_with_errors, len(parsed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data / Example use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get essays\n",
    "essay_key = pd.read_csv('../data/essays_dataset/index.csv', sep=';')\n",
    "\n",
    "essays = []\n",
    "for filename in essay_key['filename']:\n",
    "    with open('../data/essays_dataset/essays/'+filename, 'r') as f:\n",
    "        essays.append(f.read().strip())\n",
    "        \n",
    "essay_key['essay'] = essays\n",
    "essay_key.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_parses = pd.read_csv('../data/essays_dataset/index_with_parse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_essay = essay_parses.loc[0, 'parsed_essay']\n",
    "parsed_sentences = str_to_trees(parsed_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(ROOT (FRAG (CC But) (RB not) (NP (NP (DT all) (DT the) (NNS people)) (CC and) (SBAR (S (NP (DT the) (NN time)) (VP (VBZ is) (PP (IN in) (NP (NP (NN accord)) (PP (IN with) (NP (DT this) (NN problem))))) (, ,) (SBAR (IN because) (NP (NP (DT any) (NN time)) (SBAR (S (NP (DT the) (NN person)) (VP (VBZ is) (ADJP (RB too) (PP (VBG according) (PP (IN with) (NP (DT the) (NN make) (NNS products)))))))))))))) (. .)))'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_sentences[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But not all the people and the time is in accord with this problem , because any time the person is too according with the make products . "
     ]
    }
   ],
   "source": [
    "root_node = create_tree(parsed_sentences[7])\n",
    "print_leaves(root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find errors in all parsed essays\n",
    "essay_parses = pd.read_csv('../data/essays_dataset/index_with_parse.csv')\n",
    "\n",
    "for i, row in essay_parses.iterrows():\n",
    "    parsed_sentences = str_to_trees(row['parsed_essay'])\n",
    "    \n",
    "    count_sentences_with_errors = 0\n",
    "    for sent in parsed_sentences:\n",
    "        #sentence formation errors\n",
    "        errors = sentence_formation_errors(sent)\n",
    "        \n",
    "        if len(errors) > 0:\n",
    "            count_sentences_with_errors += 1\n",
    "        \n",
    "    count_sentences_with_errors, len(parsed_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:toefl]",
   "language": "python",
   "name": "conda-env-toefl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
